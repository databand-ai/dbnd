[databand]

[config]
validate_no_extra_params = warn

[core]
environments = ['local']

tracker = ['console', 'api']

tracker_api = web
tracker_version = 2

databand_url =
# credentials to connect to the databand webserver
dbnd_user = databand
dbnd_password = databand

# standalone config
fix_env_on_osx = True

[tracking]
log_histograms = True
flatten_operator_fields = {"PythonOperator": ["op_kwargs", "op_args"]}

# engines configuration
[local_machine_engine]
_type = dbnd._core.settings.engine.LocalMachineEngineConfig

[docker]
_type = dbnd_docker.docker.docker_engine_config.DockerEngineConfig
network =
sql_alchemy_conn =


[kubernetes]
_type = dbnd_docker.kubernetes.kubernetes_engine_config.KubernetesEngineConfig

pod_error_cfg_source_dict = {
        "255": {"retry_count": 3, "retry_delay": "3m"},
        "err_image_pull": {"retry_count": 0, "retry_delay": "3m"},
    }

submit_termination_grace_period = 30s

# environment configurations
[local]
root = ${DBND_HOME}/data
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = spark_local

[gcp]
_type = dbnd_gcp.env.GcpEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd

conn_id = google_cloud_default

spark_engine = dataproc

[aws]
_type = dbnd_aws.env.AwsEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = emr
docker_engine = aws_batch

[azure]
_type = dbnd_azure.env.AzureCloudConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd


# spark configurations
[spark]
_type = dbnd_spark.spark_config.SparkConfig

[livy]
_type = livy

[spark_local]
_type = dbnd_spark.local.local_spark_config.SparkLocalEngineConfig
conn_id = spark_default

[dataproc]
_type = dbnd_gcp.dataproc.dataproc_config.DataprocConfig

[databricks]
_type = dbnd_databricks.databricks_config.DatabricksConfig
conn_id = databricks_default


[qubole]
_type = dbnd_qubole.qubole_config.QuboleConfig

[databricks_azure]
local_dbfs_mount = /mnt/dbnd/

[emr]
_type = emr


[output]
path_task = {root}{sep}{env_label}{sep}{task_target_date}{sep}{task_name}{sep}{task_name}{task_class_version}_{task_signature}{sep}{output_name}{output_ext}
path_prod_immutable_task = {root}{sep}production{sep}{task_name}{task_class_version}{sep}{output_name}{output_ext}{sep}date={task_target_date}

target = csv
str = txt
object = pickle
List[object] = pickle
List[str] = csv
Dict[Any,DataFrame] = pickle
pandas_dataframe = csv
tensorflow_model = tfmodel
tensorflow_history = tfhistory

pandas_df_dict = hdf5
numpy_ndarray = numpy
matplotlib_figure = png
spark_dataframe = csv

hdf_format = fixed

validate_no_extra_params = disabled

[task]
task_env = local
task_target_date = today
task_version = 1
task_enabled = True
task_enabled_in_prod = True

task_in_memory_outputs = False
task_is_dynamic = False

task_supports_dynamic_tasks = True

task_retries = 0
task_retry_delay = 1s

validate_no_extra_params = error

[run]
heartbeat_timeout_s = 900
heartbeat_interval_s = 5
heartbeat_sender_log_to_file = True

[log]
# Logging level
level = INFO

# Logging format
formatter = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
formatter_simple = %%(asctime)s %%(levelname)s - %%(message)s
formatter_colorlog = [%%(asctime)s] %%(log_color)s%%(levelname)s %%(reset)s %%(task)-15s - %%(message)s

console_formatter_name = formatter_colorlog
file_formatter_name = formatter

sentry_url =

at_warn = azure.storage,flask_appbuilder

[airflow]
sql_alchemy_conn = dbnd
fernet_key = dbnd

enable_dbnd_context_vars = True
enable_windows_support = False

auto_add_versioned_dags = True
auto_add_scheduled_dags = True
auto_disable_scheduled_dags_load = True

optimize_airflow_db_access = True
disable_db_ping_on_connect = True
disable_dag_concurrency_rules = True
dbnd_pool = dbnd_pool

dbnd_dag_concurrency = 100000

webserver_url = http://localhost:8082

use_connections = False

[scheduler]
config_file = ${DBND_SYSTEM}/scheduler.yml
default_retries = 3
refresh_interval = 10
active_by_default = True
shell_cmd = True

[webserver]
web_server_host = 0.0.0.0
web_server_port = 8080
web_server_master_timeout = 120
web_server_worker_timeout = 120
workers = 4
worker_class = sync
worker_refresh_batch_size = 0
worker_refresh_interval = 30
access_logfile = -
error_logfile = -
access_logformat = %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s %%(L)s "%%(f)s"

# Secret key used to run your flask app
secret_key = temporary_key

# TODO: fill in the config object with all the stuff we have here
validate_no_extra_params = disabled

[airflow_monitor]
interval = 10
fetcher = web
;fetcher = db
include_logs = False
include_task_args = False
fetch_quantity = 100
oldest_incomplete_data_in_days = 14

# DAGs to monitor, if not defined - monitors all DAGs
;dag_ids = ['ingest_data_dag', 'simple_dag']

operator_user_kwargs = {"DAG": "_dag_id", "PythonOperator": "python_callable", "BashOperator": "bash_command", "DummyOperator": "trigger_rule"}

# For 'fetcher = db' mode
local_dag_folder = /usr/local/airflow/dags
sql_alchemy_conn = sqlite:////usr/local/airflow/airflow.db
rbac_username =
rbac_password =

allow_duplicates = False
debug_sync_log_dir_path =
; sql_alchemy_conn = postgresql+psycopg2://postgres:airflow@localhost:5432/airflow

# For 'fetcher = file' mode
;json_file_path =

[histogram]
;spark_parquet_cache_dir = "hdfs://tmp/"
spark_cache_dataframe = False
spark_cache_dataframe_column = True

[describe]
console_value_preview_size = 1500
