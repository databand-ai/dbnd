[databand]

[config]
validate_no_extra_params = warn

[core]
environments = ['local']

tracker = ['console', 'api']

tracker_api = web
tracker_version = 2

databand_url =
# credentials to connect to the databand webserver
dbnd_user = databand
dbnd_password = databand

# standalone config
fix_env_on_osx = True

[tracking]
log_histograms = False
flatten_operator_fields = {"PythonOperator": ["op_kwargs", "op_args"]}
track_source_code = True

[tracking_spark]
_type = dbnd_airflow.tracking.config.TrackingSparkConfig

# engines configuration
[local_machine_engine]
_type = dbnd._core.settings.engine.LocalMachineEngineConfig

[docker]
_type = dbnd_docker.docker.docker_engine_config.DockerEngineConfig
network =
sql_alchemy_conn =


[kubernetes]
_type = dbnd_docker.kubernetes.kubernetes_engine_config.KubernetesEngineConfig

pod_error_cfg_source_dict = {
        "255": {"retry_count": 3, "retry_delay": "3m"},
        "err_image_pull": {"retry_count": 0, "retry_delay": "3m"},
    }

submit_termination_grace_period = 30s

# environment configurations
[local]
root = ${DBND_HOME}/data
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = spark_local

[gcp]
_type = dbnd_gcp.env.GcpEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd

conn_id = google_cloud_default

spark_engine = dataproc

[aws]
_type = dbnd_aws.env.AwsEnvConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = emr
docker_engine = aws_batch

[azure]
_type = dbnd_azure.env.AzureCloudConfig
dbnd_local_root = ${DBND_HOME}/data/dbnd


# spark configurations
[spark]
_type = dbnd_spark.spark_config.SparkConfig

[livy]
_type = livy

[spark_local]
_type = dbnd_spark.local.local_spark_config.SparkLocalEngineConfig
conn_id = spark_default

[dataproc]
_type = dbnd_gcp.dataproc.dataproc_config.DataprocConfig

[databricks]
_type = dbnd_databricks.databricks_config.DatabricksConfig
conn_id = databricks_default


[qubole]
_type = dbnd_qubole.qubole_config.QuboleConfig

[databricks_azure]
local_dbfs_mount = /mnt/dbnd/

[emr]
_type = emr


[output]
path_task = {root}{sep}{env_label}{sep}{task_target_date}{sep}{task_name}{sep}{task_name}{task_class_version}_{task_signature}{sep}{output_name}{output_ext}
path_prod_immutable_task = {root}{sep}production{sep}{task_name}{task_class_version}{sep}{output_name}{output_ext}{sep}date={task_target_date}

target = csv
str = txt
object = pickle
List[object] = pickle
List[str] = csv
Dict[Any,DataFrame] = pickle
pandas_dataframe = csv
tensorflow_model = tfmodel
tensorflow_history = tfhistory

pandas_df_dict = hdf5
numpy_ndarray = numpy
matplotlib_figure = png
spark_dataframe = csv

hdf_format = fixed

validate_no_extra_params = disabled

[run]
# heartbeat timeout - 2 hours
heartbeat_timeout_s = 7200
heartbeat_interval_s = 5
heartbeat_sender_log_to_file = True

[log]
# Logging level
level = INFO

# Logging format
formatter = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
formatter_simple = %%(asctime)s %%(levelname)s - %%(message)s
formatter_colorlog = [%%(asctime)s] %%(log_color)s%%(levelname)s %%(reset)s %%(task)-15s - %%(message)s

console_formatter_name = formatter_colorlog
file_formatter_name = formatter

sentry_url =

at_warn = azure.storage,flask_appbuilder

[airflow]
enable_dbnd_context_vars = True
enable_windows_support = False

auto_add_versioned_dags = True
auto_add_scheduled_dags = True
auto_disable_scheduled_dags_load = True

optimize_airflow_db_access = True
disable_db_ping_on_connect = True
disable_dag_concurrency_rules = True
dbnd_pool = dbnd_pool

dbnd_dag_concurrency = 100000

webserver_url = http://localhost:8082

use_connections = False

[scheduler]
default_retries = 3
refresh_interval = 10
active_by_default = True
shell_cmd = True

[airflow_monitor]
interval = 5

# For 'fetcher = db' mode
local_dag_folder = /usr/local/airflow/dags
; sql_alchemy_conn = sqlite:////usr/local/airflow/airflow.db

# For rbac mode
rbac_username = databand
rbac_password = databand

# For 'fetcher = file' mode
;json_file_path =
;prometheus_port = 8000

[histogram]
;spark_parquet_cache_dir = "hdfs://tmp/"
spark_cache_dataframe = False
spark_cache_dataframe_column = True

[describe]
console_value_preview_size = 1500
