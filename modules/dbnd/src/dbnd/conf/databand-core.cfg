[databand]

[config]
validate_no_extra_params = warn

[core]
environments = ['local']
sql_alchemy_conn = sqlite:///${DBND_SYSTEM}/dbnd.db

tracker = ['console', 'file', 'api']
tracker_api = web

databand_url =
tracker_version = 2

# standalone config
fix_requests_on_osx = True

# engines configuration
[local_machine_engine]
_type = local_machine

[docker]
_type = docker
network =
sql_alchemy_conn =


[kubernetes]
_type = kubernetes

# assumes a Kubernetes deployement using our deploy script with an in cluster Postgres and a secret with these names
system_secrets = [{ "type":"env", "target": "AIRFLOW__CORE__SQL_ALCHEMY_CONN", "secret" : "databand-secrets" , "key" :"sql_alchemy_conn"},
                 { "type":"env", "target": "DBND__CORE__SQL_ALCHEMY_CONN", "secret" : "databand-secrets" , "key" :"sql_alchemy_conn"},
                 { "type":"env", "target": "DBND__CORE__DATABAND_URL", "secret" : "databand-secrets" , "key" :"databand_url"},
                 { "type":"env", "target": "AIRFLOW__CORE__FERNET_KEY", "secret" : "databand-secrets" , "key" :"fernet_key"}]

submit_termination_grace_period = 30s

# environment configurations
[local]
_type = local

root = ${DBND_HOME}/data
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = spark_local

[gcp]
_type = gcp
dbnd_local_root = ${DBND_HOME}/data/dbnd

conn_id = google_cloud_default

spark_engine = dataproc

[aws]
_type = aws
dbnd_local_root = ${DBND_HOME}/data/dbnd
spark_engine = emr
docker_engine = aws_batch

[azure]
_type = azure
dbnd_local_root = ${DBND_HOME}/data/dbnd


# spark configurations
[spark]
_type = spark

[livy]
_type = livy

[spark_local]
_type = spark_local
conn_id = spark_default

[dataproc]
_type = dataproc

[databricks]
_type = databricks
conn_id = databricks_default


[qubole]
_type = qubole

[databricks_azure]
local_dbfs_mount = /mnt/dbnd/

[emr]
_type = emr


[output]
path_task = {root}{sep}{env_label}{sep}{task_target_date}{sep}{task_name}{sep}{task_name}{task_class_version}_{task_signature}{sep}{output_name}{output_ext}
path_prod_immutable_task = {root}{sep}production{sep}{task_name}{task_class_version}{sep}{output_name}{output_ext}{sep}date={task_target_date}

target = csv
str = txt
object = pickle
List[object] = pickle
List[str] = csv
Dict[Any,DataFrame] = pickle
pandas_dataframe = csv

pandas_df_dict = hdf5
numpy_ndarray = numpy
matplotlib_figure = png
spark_dataframe = csv

hdf_format = fixed

validate_no_extra_params = disabled

[task]
task_env = local
task_target_date = today
task_version = 1
task_enabled = True
task_enabled_in_prod = True

task_in_memory_outputs = False
task_is_dynamic = False

task_supports_dynamic_tasks = True

task_retries = 0
task_retry_delay = 0s

validate_no_extra_params = error

[run]
heartbeat_timeout_s = 900
heartbeat_interval_s = 5
heartbeat_sender_log_to_file = True

[log]
# Logging level
level = INFO

# Logging format
formatter = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
formatter_simple = %%(asctime)s %%(levelname)s - %%(message)s
formatter_colorlog = [%%(asctime)s] %%(log_color)s%%(levelname)s %%(reset)s %%(task)-15s - %%(message)s

console_formatter_name = formatter_colorlog
file_formatter_name = formatter

sentry_url =

at_warn = azure.storage,flask_appbuilder

[airflow]
sql_alchemy_conn = dbnd
fernet_key = dbnd

auto_add_versioned_dags = True
auto_add_scheduled_dags = True
auto_disable_scheduled_dags_load = True

optimize_airflow_db_access = True
disable_db_ping_on_connect = True
disable_dag_concurrency_rules = True

dbnd_dag_concurrency = 100000

webserver_url = http://localhost:8082

[scheduler]
config_file = ${DBND_SYSTEM}/scheduler.yml
default_retries = 3
active_by_default = True
shell_cmd = True

# credentials to connect to the databand webserver
dbnd_user = databand
dbnd_password = databand

[webserver]
web_server_host = 0.0.0.0
web_server_port = 8081
web_server_master_timeout = 120
web_server_worker_timeout = 120
workers = 4
worker_class = sync
worker_refresh_batch_size = 0
worker_refresh_interval = 30
access_logfile = -
error_logfile = -
access_logformat = %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s %%(L)s "%%(f)s"

# Secret key used to run your flask app
secret_key = temporary_key

# TODO: fill in the config object with all the stuff we have here
validate_no_extra_params = disabled


[airflow_monitor]
interval = 10
fetcher = web
;fetcher = db
include_logs = True

# Fetch period in mins
fetch_period = 60


[airflow_monitor_web]
# For 'fetcher = web' mode
url = http://localhost:8080/admin/data_export_plugin/export_data

[airflow_monitor_db]
# For 'fetcher = db' mode
dag_folder =  /usr/local/airflow/dags
sql_alchemy_conn = sqlite:////usr/local/airflow/airflow.db
; sql_alchemy_conn = postgresql+psycopg2://postgres:airflow@localhost:5432/airflow
