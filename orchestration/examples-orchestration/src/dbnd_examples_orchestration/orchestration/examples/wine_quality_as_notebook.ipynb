{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbnd_examples_orchestration.data import data_repo\n",
    "\n",
    "\n",
    "\n",
    "# This example requires pandas, numpy, sklearn, scipy\n",
    "# Inspired by an MLFlow tutorial:\n",
    "#   https://github.com/databricks/mlflow/blob/master/example/tutorial/train.py\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dbnd import band, task, output, log_metric\n",
    "from targets import DataTarget\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbnd run -m dbnd_examples_orchestration predict_wine_quality --task-version now\n",
    "# dbnd run -m dbnd_examples_orchestration predict_wine_quality_parameter_search --task-version now\n",
    "\n",
    "\n",
    "def calculate_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we'll express PredictWineQuality as tasks and targets, it will look like this:\n",
    "![alt text](wine_quality.png)\n",
    "Tasks produce targets as outputs and consume targets as inputs. Targets can be a S3 path, a local file or a database record.\n",
    "\n",
    "The first thing we'll do is creating a task that split the data into separate train, test and validation sets.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@task( result=(\"training_set\", \"test_set\", \"validation_set\"))\n",
    "def prepare_data(raw_data: DataFrame):\n",
    "    train_df, test_df = train_test_split(raw_data)\n",
    "    test_df, validation_df = train_test_split(test_df, test_size=0.5)\n",
    "    return train_df, test_df , validation_df\n",
    "\n",
    "@task\n",
    "def calculate_alpha(alpha: float = 0.5):\n",
    "    alpha += 0.1\n",
    "    return alpha\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every task can be configurable, and it can be done by using parameters. For example, we can add alpha or l1_ratio parameter to train_model. It might look like this\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@task\n",
    "def train_model(\n",
    "    test_set: DataFrame,\n",
    "    training_set: DataFrame,\n",
    "    alpha: float = 0.5,\n",
    "    l1_ratio: float = 0.5,\n",
    ") -> ElasticNet:\n",
    "    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    lr.fit(training_set.drop([\"quality\"], 1), training_set[[\"quality\"]])\n",
    "    prediction = lr.predict(test_set.drop([\"quality\"], 1))\n",
    "\n",
    "    (rmse, mae, r2) = calculate_metrics(test_set[[\"quality\"]], prediction)\n",
    "\n",
    "    log_metric(\"alpha\", alpha)\n",
    "    log_metric(\"rmse\", rmse)\n",
    "    log_metric(\"mae\", rmse)\n",
    "    log_metric(\"r2\", r2)\n",
    "\n",
    "    logging.info(\n",
    "        \"Elasticnet model (alpha=%f, l1_ratio=%f): rmse = %f, mae = %f, r2 = %f\",\n",
    "        alpha,\n",
    "        l1_ratio,\n",
    "        rmse,\n",
    "        mae,\n",
    "        r2,\n",
    "    )\n",
    "    return lr\n",
    "\n",
    "\n",
    "@task(result=output.csv)\n",
    "def validate_model(model: ElasticNet, validation_dataset: DataFrame) -> str:\n",
    "    logger.info(\"Running validate model demo: %s\", validation_dataset)\n",
    "    # support for py3 parqeut\n",
    "    validation_dataset = validation_dataset.rename(str, axis=\"columns\")\n",
    "    validation_x = validation_dataset.drop([\"quality\"], 1)\n",
    "    validation_y = validation_dataset[[\"quality\"]]\n",
    "\n",
    "    prediction = model.predict(validation_x)\n",
    "    (rmse, mae, r2) = calculate_metrics(validation_y, prediction)\n",
    "\n",
    "    log_metric(\"rmse\", rmse)\n",
    "    log_metric(\"mae\", rmse)\n",
    "    log_metric(\"r2\", r2)\n",
    "\n",
    "    return [\"%s,%s,%s\" % (rmse, mae, r2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can put all tasks together.  We need to define tha output of the @band (model and validation) and assign them\n",
    "That's all.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@band(result=(\"model\", \"validation\"))\n",
    "def predict_wine_quality(\n",
    "    data: DataTarget = data_repo.wines,\n",
    "    alpha: float = 0.5,\n",
    "    l1_ratio: float = 0.5,\n",
    "    good_alpha: bool = False,\n",
    "):\n",
    "    training_set, test_set, validation_set = prepare_data(raw_data=data)\n",
    "    if good_alpha:\n",
    "        alpha = calculate_alpha(alpha)\n",
    "\n",
    "    model = train_model(\n",
    "        test_set=test_set,\n",
    "        training_set=training_set,\n",
    "        alpha=alpha,\n",
    "        l1_ratio=l1_ratio,\n",
    "    )\n",
    "\n",
    "    validation = validate_model(\n",
    "        model=model, validation_dataset=validation_set\n",
    "    )\n",
    "    return model, validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run pipeline with data and specific alpha (0.1)\n",
    "wine = predict_wine_quality.t(alpha=0.1, data=data_repo.wines, task_version=\"now\")\n",
    "wine.dbnd_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine.validation.read_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@band\n",
    "def predict_wine_quality_parameter_search(\n",
    "    data: DataTarget = data_repo.wines,\n",
    "    alpha_step: float = 0.3,\n",
    "    l1_ratio_step: float = 0.4,\n",
    "):\n",
    "    result = {}\n",
    "    variants = list(\n",
    "        itertools.product(np.arange(0, 1, alpha_step), np.arange(0, 1, l1_ratio_step))\n",
    "    )\n",
    "    logger.info(\"All Variants: %s\", variants)\n",
    "    for alpha_value, l1_ratio in variants:\n",
    "        exp_name = \"Predict_%f_l1_ratio_%f\" % (alpha_value, l1_ratio)\n",
    "        model, validation = predict_wine_quality(\n",
    "            data=data, alpha=alpha_value, l1_ratio=l1_ratio, task_name=exp_name\n",
    "        )\n",
    "\n",
    "        result[exp_name] = (model, validation)\n",
    "    return result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
